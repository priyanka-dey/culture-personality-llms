{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPEN_AI_API\"] = os.getenv(\"OPEN_AI_API\")\n",
    "open_ai_api_key = os.getenv(\"OPEN_AI_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \".\\Documents\"\n",
    "documents = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=open_ai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "XRef object at 2945 can not be read, some object may be missing\n",
      "XRef object at 2945 can not be read, some object may be missing\n",
      "Ignoring wrong pointing object 2268 0 (offset 1064028)\n",
      "Ignoring wrong pointing object 2269 0 (offset 1064028)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 477 0 (offset 0)\n",
      "Ignoring wrong pointing object 479 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 5 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "for folder, _, files in os.walk(root_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=open_ai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(split_docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever: BaseRetriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # Key used to store conversation history\n",
    "    return_messages=True        # Returns chat history in message format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt4(memory: ConversationBufferMemory, retriever: BaseRetriever, user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Handles the conversational aspect and integrates the retriever with OpenAI's GPT-4.\n",
    "\n",
    "    Args:\n",
    "        memory: ConversationBufferMemory to manage chat history.\n",
    "        retriever: The retriever for querying the vector store.\n",
    "        user_query: The user's current question.\n",
    "\n",
    "    Returns:\n",
    "        A response from GPT-4.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.get_relevant_documents(user_query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Retrieve conversation history from memory\n",
    "    chat_history = memory.chat_memory.messages\n",
    "\n",
    "    # Add context and chat history to the prompt\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Use the following context to answer the question:\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Conversation History:\\n\" +\n",
    "        \"\\n\".join([\n",
    "            f\"{'User' if isinstance(message, HumanMessage) else 'Assistant'}: {message.content}\"\n",
    "            for message in chat_history\n",
    "        ]) +\n",
    "        f\"\\n\\nUser: {user_query}\\nAssistant:\"\n",
    "    )\n",
    "\n",
    "    # Query GPT-4\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Add the user's query and GPT-4's response to memory\n",
    "    memory.chat_memory.add_user_message(user_query)\n",
    "    memory.chat_memory.add_ai_message(response.choices[0].message.content)\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or append to JSON file\n",
    "def save_to_json(data: dict, filename: str):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "        if \"cultural-norms\" in data:\n",
    "            existing_data.setdefault(\"cultural-norms\", []).extend(data[\"cultural-norms\"])\n",
    "        # if \"cultural_scenarios\" in data:\n",
    "        #     existing_data.setdefault(\"cultural_scenarios\", []).extend(data[\"cultural_scenarios\"])\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(existing_data, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cultural_norms(country: str) -> dict:\n",
    "    prompt = f\"Provide me 10 unique sentences highlighting the core values/important aspects of individuals living in {country}.\"\n",
    "    response = query_gpt4(memory, retriever, prompt)\n",
    "    print(response)\n",
    "    # Split the response into lines and filter undesired content\n",
    "    norms = []\n",
    "    for idx, line in enumerate(response.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        # Check if the line is valid (e.g., non-empty and not generic text)\n",
    "        if line and not line.lower().startswith(\"i don't know\") and not \"here are\" in line.lower():\n",
    "            # Optionally, check for a pattern (e.g., starting with a number or bullet)\n",
    "            if line[0].isdigit() or line.startswith(\"-\"):\n",
    "                norms.append({\"id\": len(norms) + 1, \"text\": line})\n",
    "    \n",
    "    # If no valid norms are found, return an empty list\n",
    "    if not norms:\n",
    "        print(f\"No valid norms generated for {country}.\")\n",
    "        return {\"gpt4-o prompt\": prompt, \"country\": country, \"norms\": []}\n",
    "    \n",
    "    return {\n",
    "        \"gpt4-o prompt\": prompt,\n",
    "        \"country\": country,\n",
    "        \"norms\": norms\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path: str) -> Dict:\n",
    "    \"\"\"Reads a JSON file and returns its content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def write_json_file(data: Dict, file_path: str):\n",
    "    print(\"Data: \\n\\n\\n\", data)\n",
    "    \"\"\"Writes data to a JSON file.\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "def generate_scenarios_for_norm(norms: List[Dict], country: str) -> Dict:\n",
    "    \"\"\"Generates cultural scenarios for each norm in a given list.\"\"\"\n",
    "    scenarios = []\n",
    "    \n",
    "    for i in range(len(norms)):\n",
    "        norm = norms[i]\n",
    "        prompt = f\"I would like to generate some example scenarios showing this cultural norm {norm} in {country}. Please generate 10 scenarios, detailing each scenario with up to 2 sentences. Please refrain from stating the cultural norm in the scenario.\"\n",
    "        try:\n",
    "            response = query_gpt4(memory, retriever, prompt)  # Replace with your model's call\n",
    "            generated_scenarios = response.split(\"\\n\")  # Assuming each scenario is a line\n",
    "            \n",
    "            # print(\"------------------------------------------------------------------\\n\")\n",
    "            # print(f\"Generated scenarios for norm {norms[i]}: \\n\", generated_scenarios)\n",
    "            # print(\"------------------------------------------------------------------\\n\")\n",
    "            \n",
    "            for scenario in generated_scenarios:\n",
    "                scenario = scenario.strip()\n",
    "                # Filter only valid scenarios that start with a numbered list (e.g., \"1.\", \"2.\")\n",
    "                if scenario and scenario[0].isdigit() and scenario[1] in [\".\", \")\"]:\n",
    "                    scenarios.append({\n",
    "                        \"id\": len(scenarios) + 1,\n",
    "                        \"norm-id\": norm[\"id\"],\n",
    "                        \"text\": scenario\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating scenarios for norms: {e}\")\n",
    "\n",
    "    scenarios_dict = {\n",
    "        \"prompt\": f\"I would like to generate some example scenarios showing this cultural norm in {country}. Please generate 10 scenarios, detailing each scenario with up to 2 sentences. Please refrain from stating the cultural norm in the scenario.\",\n",
    "        \"country\": country,\n",
    "        \"scenarios\": scenarios\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\\n\\n Scenarios: \", scenarios)\n",
    "    \n",
    "    return scenarios_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    country = input(\"Enter the country for which to generate cultural norms and scenarios: \").strip()\n",
    "    \n",
    "    norms_file = \"openai_cultural_norms.json\"\n",
    "    # scenarios_file = \"rag_cultural_scenarios.json\"\n",
    "\n",
    "    cultural_norms = generate_cultural_norms(country)\n",
    "    save_to_json({\"cultural-norms\": [cultural_norms]}, norms_file)\n",
    "    \n",
    "    # Read the norms from the JSON file\n",
    "    # cultural_data = read_json_file(norms_file)\n",
    "    # all_scenarios = []\n",
    "\n",
    "    # Iterate through each country's norms\n",
    "    # for country_data in cultural_data.get(\"cultural-norms\", []):\n",
    "    #     country = country_data.get(\"country\")\n",
    "    #     norms = country_data.get(\"norms\", [])\n",
    "\n",
    "    #     print(f\"The norms of {country} are: \\n\", norms)\n",
    "        \n",
    "    #     if not norms:\n",
    "    #         print(f\"No norms found for {country}.\")\n",
    "    #         continue\n",
    "\n",
    "    #     # Generate scenarios for the country's norms\n",
    "    #     country_scenarios = generate_scenarios_for_norm(norms, country)\n",
    "    #     print(\"\\n\\n\\n\\n\\n --------------------------------------------- \\n\")\n",
    "    #     print(country_scenarios)\n",
    "        \n",
    "    #     # all_scenarios.extend(country_scenarios)\n",
    "\n",
    "    # # Save all scenarios to a single JSON file\n",
    "    # write_json_file({\"cultural-scenarios\": country_scenarios}, scenarios_file)\n",
    "    # print(f\"Cultural scenarios saved to {scenarios_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. South Africa prides itself on its cultural diversity, often celebrated through the concept of a \"Rainbow Nation\" that respects and integrates a multitude of ethnic and cultural identities.\n",
      "\n",
      "2. Ubuntu, a Nguni Bantu term meaning \"humanity towards others,\" embodies the South African value of interconnectedness, emphasizing community, compassion, and mutual care.\n",
      "\n",
      "3. The legacy of apartheid has instilled a significant focus on equality and human rights in South African society, with efforts to address historical injustices and promote social justice.\n",
      "\n",
      "4. Family and extended kin networks hold a central place in South African life, providing emotional and social support across generations and playing a vital role in individual identity.\n",
      "\n",
      "5. The South African Constitution is one of the most progressive globally, enshrining a commitment to non-racialism, equality, and the protection of a broad spectrum of human rights, including cultural rights.\n",
      "\n",
      "6. South Africans often exercise a strong sense of resilience and adaptability, shaped by a history of overcoming challenges through unity and determination.\n",
      "\n",
      "7. The country's rich natural landscapes and biodiversity foster a deep cultural appreciation for nature, influencing recreational activities and environmental consciousness.\n",
      "\n",
      "8. Language diversity is a key component of cultural expression in South Africa, with 11 official languages reflecting the country's multicultural makeup and promoting linguistic inclusivity.\n",
      "\n",
      "9. Celebrations and communal gatherings, such as traditional festivals and public holidays, are central to South African cultural life, fostering unity and a shared sense of heritage among different communities.\n",
      "\n",
      "10. Economic empowerment and addressing socio-economic disparities remain significant focus areas, as South Africans strive to create opportunities for development and prosperity for all citizens.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
